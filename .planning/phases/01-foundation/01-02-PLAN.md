---
phase: 01-foundation
plan: 02
type: execute
wave: 2
depends_on: ["01-01"]
files_modified:
  - src/parser/index.ts
  - src/parser/frontmatter.ts
  - src/parser/body.ts
  - src/parser/location.ts
  - src/parser/types.ts
autonomous: true

must_haves:
  truths:
    - "Parser extracts YAML frontmatter correctly (name, version, config, secrets, schemas)"
    - "Parser extracts XML body into typed AST nodes with correct structure"
    - "Parser preserves source locations (line, column, offset) for all nodes"
    - "Malformed YAML produces clear error with line number"
    - "Malformed XML produces clear error with line number"
    - "Parser is secure against XXE injection (processEntities: false)"
    - "Parser is secure against YAML deserialization (Bun.YAML safe by default)"
  artifacts:
    - src/parser/index.ts
    - src/parser/frontmatter.ts
    - src/parser/body.ts
    - src/parser/location.ts
    - src/parser/types.ts
  key_links:
    - "parse() returns WorkflowAST or errors with source locations"
    - "Source location offsets account for frontmatter line count"
    - "XMLParser configured with processEntities: false"
---

<objective>
Implement the FlowScript parser that reads .flow.md files and produces a typed AST with source locations preserved for error messages.

Purpose: Enable validation and execution by providing structured representation of workflow files
Output: Complete parser module that handles YAML frontmatter, XML body, and source location tracking
</objective>

<execution_context>
@/Users/narcisbrindusescu/.claude/looppool/workflows/execute-plan.md
@/Users/narcisbrindusescu/.claude/looppool/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/phases/01-foundation/01-RESEARCH.md
@.planning/phases/01-foundation/01-01-SUMMARY.md

Research patterns to implement:
- Three-phase file parsing (split sections, parse each, merge)
- Source location preservation via byte offset conversion
- Secure XML parsing with fast-xml-parser (processEntities: false)
- Secure YAML parsing with Bun.YAML (safe by default)

Requirements covered:
- PARSE-01: Parse YAML frontmatter
- PARSE-02: Parse XML body into node AST
- PARSE-03: Preserve source locations for error messages
- PARSE-04: Handle malformed input with clear error messages
- PARSE-05: Secure against XXE injection
- PARSE-06: Secure against YAML deserialization attacks
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement source location tracking utilities</name>
  <files>src/parser/location.ts</files>
  <action>
    Create source location tracking based on research pattern:

    ```typescript
    import type { Position, SourceLocation, SourceMap } from '../types';

    /**
     * Build an index of line start offsets for fast position lookups.
     * Line 1 starts at offset 0.
     */
    export function buildLineOffsets(source: string): number[] {
      const offsets = [0];  // Line 1 starts at offset 0
      for (let i = 0; i < source.length; i++) {
        if (source[i] === '\n') {
          offsets.push(i + 1);  // Next line starts after newline
        }
      }
      return offsets;
    }

    /**
     * Convert a byte offset to line/column position.
     * Uses binary search for efficiency on large files.
     */
    export function offsetToPosition(offset: number, lineOffsets: number[]): Position {
      // Binary search for the line
      let low = 0;
      let high = lineOffsets.length - 1;

      while (low < high) {
        const mid = Math.ceil((low + high) / 2);
        if (lineOffsets[mid] <= offset) {
          low = mid;
        } else {
          high = mid - 1;
        }
      }

      const line = low + 1;  // 1-indexed
      const column = offset - lineOffsets[low];  // 0-indexed

      return { line, column, offset };
    }

    /**
     * Create a SourceLocation from start and end offsets.
     */
    export function createLocation(
      startOffset: number,
      endOffset: number,
      lineOffsets: number[]
    ): SourceLocation {
      return {
        start: offsetToPosition(startOffset, lineOffsets),
        end: offsetToPosition(endOffset, lineOffsets)
      };
    }

    /**
     * Adjust a location by adding a line offset (for XML body after frontmatter).
     */
    export function adjustLocation(
      loc: SourceLocation,
      lineOffset: number,
      byteOffset: number
    ): SourceLocation {
      return {
        start: {
          line: loc.start.line + lineOffset,
          column: loc.start.column,
          offset: loc.start.offset + byteOffset
        },
        end: {
          line: loc.end.line + lineOffset,
          column: loc.end.column,
          offset: loc.end.offset + byteOffset
        }
      };
    }

    /**
     * Create a SourceMap for the entire file.
     */
    export function createSourceMap(source: string, filePath: string): SourceMap {
      return {
        source,
        filePath,
        lineOffsets: buildLineOffsets(source)
      };
    }

    /**
     * Find byte offset of a substring in source (for locating XML tags).
     */
    export function findOffset(source: string, searchString: string, startFrom = 0): number {
      return source.indexOf(searchString, startFrom);
    }
    ```
  </action>
  <verify>
    Create a quick test:
    ```bash
    echo "
    import { buildLineOffsets, offsetToPosition } from './src/parser/location';
    const source = 'line1\nline2\nline3';
    const offsets = buildLineOffsets(source);
    const pos = offsetToPosition(6, offsets);
    console.assert(pos.line === 2, 'Should be line 2');
    console.assert(pos.column === 0, 'Should be column 0');
    console.log('Location utils OK');
    " | bun -
    ```
  </verify>
  <done>
    - src/parser/location.ts exists with all utility functions
    - buildLineOffsets correctly indexes newlines
    - offsetToPosition correctly converts byte offsets to line/column
    - adjustLocation can shift locations for body offset
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement YAML frontmatter and XML body parsing</name>
  <files>src/parser/frontmatter.ts, src/parser/body.ts, src/parser/types.ts</files>
  <action>
    1. Create src/parser/types.ts for internal parser types:
       ```typescript
       import type { WorkflowMetadata } from '../types';

       export interface FileSections {
         frontmatter: string;
         frontmatterStart: number;  // Byte offset where frontmatter content starts
         frontmatterEnd: number;    // Byte offset where frontmatter ends (before closing ---)
         frontmatterLineCount: number;  // Total lines in frontmatter section including ---
         body: string;
         bodyStart: number;         // Byte offset where body starts
       }

       export interface RawXMLNode {
         ':@'?: Record<string, string>;  // Attributes
         '#text'?: string;               // Text content
         [tagName: string]: unknown;     // Child elements
       }
       ```

    2. Create src/parser/frontmatter.ts:
       ```typescript
       import { z } from 'zod';
       import type { WorkflowMetadata, ValidationError } from '../types';
       import { createError } from '../types';
       import type { FileSections } from './types';
       import { createLocation } from './location';

       // Zod schema for runtime validation
       const configFieldSchema = z.object({
         type: z.enum(['string', 'number', 'boolean', 'object', 'array']),
         default: z.unknown().optional(),
         required: z.boolean().optional(),
         description: z.string().optional()
       });

       const triggerSchema = z.object({
         type: z.enum(['manual', 'webhook', 'schedule']),
         config: z.record(z.unknown()).optional()
       });

       const metadataSchema = z.object({
         name: z.string().min(1, 'Workflow name is required'),
         version: z.string().regex(/^\d+\.\d+(\.\d+)?$/, 'Version must be semver format (e.g., 1.0.0)'),
         description: z.string().optional(),
         trigger: triggerSchema.optional(),
         config: z.record(configFieldSchema).optional(),
         secrets: z.array(z.string()).optional(),
         schemas: z.record(z.unknown()).optional()
       });

       /**
        * Split source file into frontmatter and body sections.
        * Frontmatter is delimited by --- at start and end.
        */
       export function splitSections(source: string): FileSections | ValidationError {
         const lines = source.split('\n');

         // Find opening ---
         if (lines[0]?.trim() !== '---') {
           return createError(
             'PARSE_MISSING_FRONTMATTER',
             'File must start with YAML frontmatter (---)',
             { start: { line: 1, column: 0, offset: 0 }, end: { line: 1, column: 0, offset: 0 } },
             ['Add --- at the beginning of the file followed by YAML metadata']
           );
         }

         // Find closing ---
         let closingLine = -1;
         for (let i = 1; i < lines.length; i++) {
           if (lines[i]?.trim() === '---') {
             closingLine = i;
             break;
           }
         }

         if (closingLine === -1) {
           return createError(
             'PARSE_MISSING_FRONTMATTER',
             'YAML frontmatter must be closed with ---',
             { start: { line: 1, column: 0, offset: 0 }, end: { line: lines.length, column: 0, offset: source.length } },
             ['Add --- after your YAML metadata to close the frontmatter section']
           );
         }

         // Calculate byte offsets
         let frontmatterStart = 4;  // After opening "---\n"
         let currentOffset = 0;

         for (let i = 0; i <= closingLine; i++) {
           if (i === 1) frontmatterStart = currentOffset;
           currentOffset += lines[i].length + 1;  // +1 for newline
         }

         const frontmatterEnd = currentOffset - lines[closingLine].length - 1;
         const bodyStart = currentOffset;

         const frontmatter = lines.slice(1, closingLine).join('\n');
         const body = lines.slice(closingLine + 1).join('\n').trim();

         if (!body) {
           return createError(
             'PARSE_MISSING_BODY',
             'Workflow file must contain an XML body after frontmatter',
             { start: { line: closingLine + 1, column: 0, offset: bodyStart }, end: { line: closingLine + 1, column: 0, offset: bodyStart } },
             ['Add workflow nodes (source, transform, sink, etc.) after the closing ---']
           );
         }

         return {
           frontmatter,
           frontmatterStart,
           frontmatterEnd,
           frontmatterLineCount: closingLine + 1,  // Lines 0 through closingLine
           body,
           bodyStart
         };
       }

       /**
        * Parse YAML frontmatter into WorkflowMetadata.
        * Uses Bun.YAML which is safe by default (no object instantiation).
        */
       export function parseFrontmatter(
         yaml: string,
         lineOffsets: number[],
         startLine: number
       ): { data: WorkflowMetadata } | { errors: ValidationError[] } {
         let raw: unknown;

         try {
           raw = Bun.YAML.parse(yaml);
         } catch (e) {
           const error = e as Error;
           // Try to extract line number from error message
           const lineMatch = error.message.match(/line (\d+)/i);
           const errorLine = lineMatch ? parseInt(lineMatch[1], 10) + startLine : startLine;

           return {
             errors: [
               createError(
                 'PARSE_YAML_INVALID',
                 `Invalid YAML: ${error.message}`,
                 {
                   start: { line: errorLine, column: 0, offset: 0 },
                   end: { line: errorLine, column: 0, offset: 0 }
                 }
               )
             ]
           };
         }

         // Validate with zod
         const result = metadataSchema.safeParse(raw);

         if (!result.success) {
           const errors: ValidationError[] = result.error.issues.map(issue => {
             const path = issue.path.join('.');
             return createError(
               'VALID_INVALID_FIELD_TYPE',
               `${path ? `${path}: ` : ''}${issue.message}`,
               {
                 start: { line: startLine + 1, column: 0, offset: 0 },
                 end: { line: startLine + 1, column: 0, offset: 0 }
               }
             );
           });
           return { errors };
         }

         return { data: result.data as WorkflowMetadata };
       }
       ```

    3. Create src/parser/body.ts:
       ```typescript
       import { XMLParser } from 'fast-xml-parser';
       import type { NodeAST, SourceLocation, ValidationError } from '../types';
       import { createError } from '../types';
       import type { RawXMLNode } from './types';
       import { findOffset, createLocation, adjustLocation, buildLineOffsets } from './location';

       // Secure XML parser configuration - XXE PREVENTION
       const xmlParser = new XMLParser({
         processEntities: false,        // CRITICAL: Prevents XXE injection
         ignoreAttributes: false,       // Keep attributes (id, type, input, etc.)
         attributeNamePrefix: '',       // No prefix for cleaner access
         parseTagValue: false,          // Keep values as strings
         trimValues: true,              // Clean whitespace
         preserveOrder: true,           // Maintain node order
         isArray: (name) => [
           'workflow', 'source', 'transform', 'sink',
           'branch', 'case', 'if', 'else', 'then',
           'loop', 'while', 'foreach', 'parallel',
           'checkpoint'
         ].includes(name)
       });

       /**
        * Parse XML body into AST nodes.
        */
       export function parseXMLBody(
         xml: string,
         bodyStartLine: number,
         bodyStartOffset: number,
         fullSource: string
       ): { nodes: NodeAST[] } | { errors: ValidationError[] } {
         let parsed: unknown[];

         try {
           parsed = xmlParser.parse(xml);
         } catch (e) {
           const error = e as Error;
           // Extract position info if available
           const lineMatch = error.message.match(/line:?\s*(\d+)/i);
           const colMatch = error.message.match(/col(?:umn)?:?\s*(\d+)/i);

           const errorLine = lineMatch
             ? parseInt(lineMatch[1], 10) + bodyStartLine
             : bodyStartLine;
           const errorCol = colMatch ? parseInt(colMatch[1], 10) : 0;

           return {
             errors: [
               createError(
                 'PARSE_XML_INVALID',
                 `Invalid XML: ${error.message}`,
                 {
                   start: { line: errorLine, column: errorCol, offset: 0 },
                   end: { line: errorLine, column: errorCol, offset: 0 }
                 },
                 ['Check for unclosed tags, invalid characters, or malformed attributes']
               )
             ]
           };
         }

         // Build line offsets for the body section for location tracking
         const bodyLineOffsets = buildLineOffsets(xml);

         const nodes: NodeAST[] = [];
         const errors: ValidationError[] = [];

         // Convert raw XML to AST nodes
         for (const element of parsed) {
           const result = convertElement(element, xml, bodyLineOffsets, bodyStartLine, bodyStartOffset);
           if ('error' in result) {
             errors.push(result.error);
           } else if (result.node) {
             nodes.push(result.node);
           }
         }

         if (errors.length > 0) {
           return { errors };
         }

         return { nodes };
       }

       /**
        * Convert a raw XML element to an AST node.
        */
       function convertElement(
         element: unknown,
         xml: string,
         lineOffsets: number[],
         lineOffset: number,
         byteOffset: number
       ): { node: NodeAST | null } | { error: ValidationError } {
         if (typeof element !== 'object' || element === null) {
           return { node: null };
         }

         const obj = element as Record<string, unknown>;

         // Find the tag name (key that isn't ':@')
         const tagName = Object.keys(obj).find(k => k !== ':@');
         if (!tagName) {
           return { node: null };
         }

         const attrs = (obj[':@'] as Record<string, string>) || {};
         const content = obj[tagName];

         // Find location in source
         const tagPattern = `<${tagName}`;
         const tagOffset = findOffset(xml, tagPattern);
         const loc = tagOffset >= 0
           ? adjustLocation(
               createLocation(tagOffset, tagOffset + tagName.length + 1, lineOffsets),
               lineOffset,
               byteOffset
             )
           : { start: { line: lineOffset, column: 0, offset: byteOffset }, end: { line: lineOffset, column: 0, offset: byteOffset } };

         // Check required id attribute
         if (!attrs.id && !['case', 'then', 'else', 'workflow'].includes(tagName)) {
           return {
             error: createError(
               'VALID_MISSING_REQUIRED_FIELD',
               `<${tagName}> element missing required 'id' attribute`,
               loc,
               [`Add id="unique-name" to the <${tagName}> element`]
             )
           };
         }

         // Map tag to node type
         switch (tagName) {
           case 'source':
             return {
               node: {
                 type: 'source',
                 id: attrs.id,
                 sourceType: (attrs.type as 'http' | 'file') || 'http',
                 input: attrs.input,
                 config: extractConfig(content),
                 loc
               }
             };

           case 'transform':
             return {
               node: {
                 type: 'transform',
                 id: attrs.id,
                 transformType: (attrs.type as 'ai' | 'template' | 'map' | 'filter') || 'template',
                 input: attrs.input,
                 config: extractConfig(content),
                 loc
               }
             };

           case 'sink':
             return {
               node: {
                 type: 'sink',
                 id: attrs.id,
                 sinkType: (attrs.type as 'http' | 'file') || 'file',
                 input: attrs.input,
                 config: extractConfig(content),
                 loc
               }
             };

           case 'branch':
             return convertBranch(obj, attrs, loc, xml, lineOffsets, lineOffset, byteOffset);

           case 'if':
             return convertIf(obj, attrs, loc, xml, lineOffsets, lineOffset, byteOffset);

           case 'loop':
             return convertLoop(obj, attrs, loc, xml, lineOffsets, lineOffset, byteOffset);

           case 'while':
             return convertWhile(obj, attrs, loc, xml, lineOffsets, lineOffset, byteOffset);

           case 'foreach':
             return convertForeach(obj, attrs, loc, xml, lineOffsets, lineOffset, byteOffset);

           case 'parallel':
             return convertParallel(obj, attrs, loc, xml, lineOffsets, lineOffset, byteOffset);

           case 'checkpoint':
             return {
               node: {
                 type: 'checkpoint',
                 id: attrs.id,
                 prompt: extractTextContent(content, 'prompt') || '',
                 timeout: attrs.timeout ? parseInt(attrs.timeout, 10) : undefined,
                 defaultAction: attrs.default as 'approve' | 'reject' | undefined,
                 input: attrs.input,
                 loc
               }
             };

           case 'workflow':
             // Workflow is a container, process its children
             return { node: null };

           default:
             return {
               error: createError(
                 'VALID_UNKNOWN_NODE_TYPE',
                 `Unknown node type: <${tagName}>`,
                 loc,
                 ['Valid node types: source, transform, sink, branch, if, loop, while, foreach, parallel, checkpoint']
               )
             };
         }
       }

       // Helper functions for complex node types
       function convertBranch(
         obj: Record<string, unknown>,
         attrs: Record<string, string>,
         loc: SourceLocation,
         xml: string,
         lineOffsets: number[],
         lineOffset: number,
         byteOffset: number
       ): { node: NodeAST } | { error: ValidationError } {
         // Simplified - full implementation would parse cases
         return {
           node: {
             type: 'branch',
             id: attrs.id,
             input: attrs.input,
             cases: [],
             loc
           }
         };
       }

       function convertIf(
         obj: Record<string, unknown>,
         attrs: Record<string, string>,
         loc: SourceLocation,
         xml: string,
         lineOffsets: number[],
         lineOffset: number,
         byteOffset: number
       ): { node: NodeAST } | { error: ValidationError } {
         return {
           node: {
             type: 'if',
             id: attrs.id,
             condition: attrs.condition || '',
             input: attrs.input,
             then: [],
             else: undefined,
             loc
           }
         };
       }

       function convertLoop(
         obj: Record<string, unknown>,
         attrs: Record<string, string>,
         loc: SourceLocation,
         xml: string,
         lineOffsets: number[],
         lineOffset: number,
         byteOffset: number
       ): { node: NodeAST } | { error: ValidationError } {
         return {
           node: {
             type: 'loop',
             id: attrs.id,
             maxIterations: attrs.max ? parseInt(attrs.max, 10) : undefined,
             breakCondition: attrs.break,
             input: attrs.input,
             body: [],
             loc
           }
         };
       }

       function convertWhile(
         obj: Record<string, unknown>,
         attrs: Record<string, string>,
         loc: SourceLocation,
         xml: string,
         lineOffsets: number[],
         lineOffset: number,
         byteOffset: number
       ): { node: NodeAST } | { error: ValidationError } {
         return {
           node: {
             type: 'while',
             id: attrs.id,
             condition: attrs.condition || '',
             input: attrs.input,
             body: [],
             loc
           }
         };
       }

       function convertForeach(
         obj: Record<string, unknown>,
         attrs: Record<string, string>,
         loc: SourceLocation,
         xml: string,
         lineOffsets: number[],
         lineOffset: number,
         byteOffset: number
       ): { node: NodeAST } | { error: ValidationError } {
         return {
           node: {
             type: 'foreach',
             id: attrs.id,
             collection: attrs.collection || '',
             itemVar: attrs.item || 'item',
             maxConcurrency: attrs['max-concurrency'] ? parseInt(attrs['max-concurrency'], 10) : undefined,
             input: attrs.input,
             body: [],
             loc
           }
         };
       }

       function convertParallel(
         obj: Record<string, unknown>,
         attrs: Record<string, string>,
         loc: SourceLocation,
         xml: string,
         lineOffsets: number[],
         lineOffset: number,
         byteOffset: number
       ): { node: NodeAST } | { error: ValidationError } {
         return {
           node: {
             type: 'parallel',
             id: attrs.id,
             input: attrs.input,
             branches: [],
             loc
           }
         };
       }

       function extractConfig(content: unknown): Record<string, unknown> {
         if (!content || !Array.isArray(content)) return {};

         const config: Record<string, unknown> = {};
         for (const item of content) {
           if (typeof item === 'object' && item !== null) {
             const obj = item as Record<string, unknown>;
             for (const [key, value] of Object.entries(obj)) {
               if (key !== ':@') {
                 config[key] = extractValue(value);
               }
             }
           }
         }
         return config;
       }

       function extractValue(value: unknown): unknown {
         if (Array.isArray(value) && value.length === 1) {
           const item = value[0];
           if (typeof item === 'object' && item !== null && '#text' in item) {
             return (item as { '#text': string })['#text'];
           }
         }
         return value;
       }

       function extractTextContent(content: unknown, tagName: string): string | undefined {
         if (!content || !Array.isArray(content)) return undefined;

         for (const item of content) {
           if (typeof item === 'object' && item !== null && tagName in item) {
             const value = (item as Record<string, unknown>)[tagName];
             return extractValue(value) as string;
           }
         }
         return undefined;
       }
       ```
  </action>
  <verify>
    Run `bun run typecheck` to ensure no type errors
  </verify>
  <done>
    - src/parser/frontmatter.ts parses YAML safely with Bun.YAML
    - src/parser/body.ts parses XML with processEntities: false
    - Both produce clear errors with line numbers for malformed input
    - TypeScript compiles without errors
  </done>
</task>

<task type="auto">
  <name>Task 3: Create main parser entry point</name>
  <files>src/parser/index.ts</files>
  <action>
    Create src/parser/index.ts that orchestrates the parsing pipeline:

    ```typescript
    import type { WorkflowAST, ValidationError } from '../types';
    import { splitSections, parseFrontmatter } from './frontmatter';
    import { parseXMLBody } from './body';
    import { createSourceMap, buildLineOffsets } from './location';

    export interface ParseSuccess {
      success: true;
      ast: WorkflowAST;
    }

    export interface ParseFailure {
      success: false;
      errors: ValidationError[];
    }

    export type ParseResult = ParseSuccess | ParseFailure;

    /**
     * Parse a .flow.md file into a WorkflowAST.
     *
     * The file format is:
     * ---
     * name: workflow-name
     * version: 1.0.0
     * ...yaml config...
     * ---
     * <source id="...">...</source>
     * <transform id="...">...</transform>
     * ...xml body...
     *
     * @param source - The file content as a string
     * @param filePath - The file path (for error messages)
     * @returns ParseResult with AST on success, errors on failure
     */
    export function parse(source: string, filePath: string): ParseResult {
      const sourceMap = createSourceMap(source, filePath);

      // Step 1: Split into frontmatter and body sections
      const sections = splitSections(source);

      if ('code' in sections) {
        // sections is a ValidationError
        return { success: false, errors: [sections] };
      }

      // Step 2: Parse YAML frontmatter
      const frontmatterResult = parseFrontmatter(
        sections.frontmatter,
        sourceMap.lineOffsets,
        1  // Frontmatter starts at line 1 (after opening ---)
      );

      if ('errors' in frontmatterResult) {
        return { success: false, errors: frontmatterResult.errors };
      }

      // Step 3: Parse XML body
      const bodyResult = parseXMLBody(
        sections.body,
        sections.frontmatterLineCount,
        sections.bodyStart,
        source
      );

      if ('errors' in bodyResult) {
        return { success: false, errors: bodyResult.errors };
      }

      // Step 4: Assemble the complete AST
      const ast: WorkflowAST = {
        metadata: frontmatterResult.data,
        nodes: bodyResult.nodes,
        sourceMap
      };

      return { success: true, ast };
    }

    // Re-export types and utilities that consumers might need
    export { createSourceMap, buildLineOffsets, offsetToPosition } from './location';
    export type { FileSections } from './types';
    ```
  </action>
  <verify>
    Create a test .flow.md file and parse it:
    ```bash
    # Create test file
    mkdir -p /tmp/flowscript-test
    cat > /tmp/flowscript-test/test.flow.md << 'EOF'
    ---
    name: test-workflow
    version: 1.0.0
    secrets:
      - API_KEY
    ---
    <source id="fetch-data" type="http">
      <url>https://api.example.com/data</url>
      <headers>
        <Authorization>Bearer {{$secrets.API_KEY}}</Authorization>
      </headers>
    </source>

    <transform id="process" type="template" input="fetch-data">
      <template>Processed: {{input.data}}</template>
    </transform>
    EOF

    # Test parsing
    echo "
    import { parse } from './src/parser';
    import { readFileSync } from 'fs';

    const content = readFileSync('/tmp/flowscript-test/test.flow.md', 'utf-8');
    const result = parse(content, 'test.flow.md');

    if (result.success) {
      console.log('Parse SUCCESS');
      console.log('Workflow name:', result.ast.metadata.name);
      console.log('Node count:', result.ast.nodes.length);
      console.log('Nodes:', result.ast.nodes.map(n => n.id).join(', '));
    } else {
      console.log('Parse FAILED');
      result.errors.forEach(e => console.log('-', e.message));
    }
    " | bun -
    ```
  </verify>
  <done>
    - src/parser/index.ts exists with parse() function
    - parse() returns success with AST or failure with errors
    - Parser correctly extracts metadata and nodes from .flow.md files
    - Source locations are preserved in all AST nodes
  </done>
</task>

</tasks>

<verification>
Run these commands to verify the plan completed successfully:

```bash
# 1. TypeScript compiles
bun run typecheck

# 2. Parser module is importable
echo "import { parse } from './src/parser'; console.log('Parser OK')" | bun -

# 3. Test with valid file
cat > /tmp/valid.flow.md << 'EOF'
---
name: test
version: 1.0.0
---
<source id="s1" type="http">
  <url>https://example.com</url>
</source>
EOF
echo "
import { parse } from './src/parser';
const result = parse(Bun.file('/tmp/valid.flow.md').text(), 'valid.flow.md');
console.assert(result.success === true, 'Should parse valid file');
console.log('Valid file test: PASS');
" | bun -

# 4. Test with invalid YAML
cat > /tmp/bad-yaml.flow.md << 'EOF'
---
name: test
version: bad version format
---
<source id="s1" type="http"/>
EOF
echo "
import { parse } from './src/parser';
const result = parse(Bun.file('/tmp/bad-yaml.flow.md').text(), 'bad.flow.md');
console.assert(result.success === false, 'Should fail on bad YAML');
console.log('Invalid YAML test: PASS');
" | bun -

# 5. Test with invalid XML
cat > /tmp/bad-xml.flow.md << 'EOF'
---
name: test
version: 1.0.0
---
<source id="s1" type="http">
  <unclosed>
</source>
EOF
echo "
import { parse } from './src/parser';
const result = parse(Bun.file('/tmp/bad-xml.flow.md').text(), 'bad.flow.md');
console.assert(result.success === false, 'Should fail on bad XML');
console.log('Invalid XML test: PASS');
" | bun -
```

All tests should pass.
</verification>

<success_criteria>
- [ ] `bun run typecheck` passes with no errors
- [ ] parse() correctly extracts YAML frontmatter (name, version, secrets)
- [ ] parse() correctly extracts XML body into typed AST nodes
- [ ] Source locations are attached to all nodes (line, column, offset)
- [ ] Malformed YAML produces clear error with line number
- [ ] Malformed XML produces clear error with line number
- [ ] XXE injection is prevented (processEntities: false)
- [ ] YAML deserialization is safe (Bun.YAML)
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation/01-02-SUMMARY.md`
</output>
