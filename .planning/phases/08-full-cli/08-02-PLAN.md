---
phase: 08-full-cli
plan: 02
type: execute
wave: 2
depends_on: ["08-01"]
files_modified:
  - src/cli/run.test.ts
autonomous: true

must_haves:
  truths:
    - "Tests verify run command parses files correctly"
    - "Tests verify dry-run shows execution plan"
    - "Tests verify config overrides merge with frontmatter config"
    - "Tests verify input JSON is available in execution context"
    - "Tests verify error cases return proper exit status"
  artifacts:
    - src/cli/run.test.ts
    - src/cli/fixtures/test-workflow.flow.md
  key_links:
    - "run.test.ts imports runWorkflow from ./run"
---

<objective>
Add comprehensive tests for the run command implementation.

Purpose: Ensure the run command handles all edge cases correctly, including config overrides, input parsing, dry-run mode, and error scenarios. Tests provide confidence for the final phase of FlowScript v1.0.

Output: Test file covering run command functionality with passing tests.
</objective>

<execution_context>
@/Users/narcisbrindusescu/.claude/looppool/workflows/execute-plan.md
@/Users/narcisbrindusescu/.claude/looppool/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/phases/08-full-cli/08-01-SUMMARY.md

# Implementation to test
@src/cli/run.ts

# Test patterns from codebase
@src/expression/evaluator.test.ts
@src/execution/executor.test.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create test workflow fixture</name>
  <files>src/cli/fixtures/test-workflow.flow.md</files>
  <action>
First create the fixtures directory, then create the test workflow file:

```bash
mkdir -p src/cli/fixtures
```

Then create `src/cli/fixtures/test-workflow.flow.md`:

```markdown
---
name: test-workflow
version: 1.0.0
config:
  output_dir: ./default
  timeout: 30
---

<source id="data" type="http">
  <url>https://api.example.com/data</url>
</source>

<transform id="process" type="template" input="data">
  <template>Result: {{input.value}}</template>
</transform>
```

This workflow is simple enough to test parsing/validation but has config values that can be overridden.
  </action>
  <verify>File exists and is valid: `bun src/cli/index.ts validate src/cli/fixtures/test-workflow.flow.md`</verify>
  <done>Test workflow file created and passes validation</done>
</task>

<task type="auto">
  <name>Task 2: Create run command tests</name>
  <files>src/cli/run.test.ts</files>
  <action>
Create comprehensive tests for the run command:

```typescript
import { test, expect, describe, beforeEach, afterEach } from 'bun:test';
import { runWorkflow, type RunOptions } from './run';

const TEST_WORKFLOW = 'src/cli/fixtures/test-workflow.flow.md';

describe('runWorkflow', () => {
  describe('file handling', () => {
    test('returns error for nonexistent file', async () => {
      const result = await runWorkflow('nonexistent.flow.md');
      expect(result.success).toBe(false);
      expect(result.output).toContain('not found');
    });

    test('returns error for invalid workflow', async () => {
      // Create temp file with invalid content
      const tmpPath = '/tmp/invalid-workflow.flow.md';
      await Bun.write(tmpPath, 'not valid yaml or xml');

      const result = await runWorkflow(tmpPath);
      expect(result.success).toBe(false);
    });
  });

  describe('dry-run mode', () => {
    test('shows execution plan without executing', async () => {
      const result = await runWorkflow(TEST_WORKFLOW, { dryRun: true });
      expect(result.success).toBe(true);
      expect(result.output).toContain('Execution Plan');
      expect(result.output).toContain('Wave');
      expect(result.output).toContain('test-workflow');
    });

    test('shows node count in plan', async () => {
      const result = await runWorkflow(TEST_WORKFLOW, { dryRun: true });
      expect(result.output).toMatch(/\d+ nodes?/);
    });
  });

  describe('config overrides', () => {
    test('parses single config override', async () => {
      const result = await runWorkflow(TEST_WORKFLOW, {
        dryRun: true,
        config: ['output_dir=./custom'],
      });
      expect(result.success).toBe(true);
    });

    test('parses multiple config overrides', async () => {
      const result = await runWorkflow(TEST_WORKFLOW, {
        dryRun: true,
        config: ['output_dir=./custom', 'timeout=60'],
      });
      expect(result.success).toBe(true);
    });

    test('errors on invalid config format', async () => {
      const result = await runWorkflow(TEST_WORKFLOW, {
        dryRun: true,
        config: ['invalid-no-equals'],
      });
      expect(result.success).toBe(false);
      expect(result.output).toContain('Invalid config format');
    });

    test('parses JSON values in config', async () => {
      const result = await runWorkflow(TEST_WORKFLOW, {
        dryRun: true,
        config: ['nested={"key": "value"}'],
      });
      expect(result.success).toBe(true);
    });
  });

  describe('input parsing', () => {
    test('parses valid JSON input', async () => {
      const result = await runWorkflow(TEST_WORKFLOW, {
        dryRun: true,
        input: '{"name": "test", "count": 42}',
      });
      expect(result.success).toBe(true);
    });

    test('errors on invalid JSON input', async () => {
      const result = await runWorkflow(TEST_WORKFLOW, {
        dryRun: true,
        input: 'not valid json',
      });
      expect(result.success).toBe(false);
      expect(result.output).toContain('Invalid JSON');
    });
  });

  describe('output format', () => {
    test('text format is default', async () => {
      const result = await runWorkflow(TEST_WORKFLOW, { dryRun: true });
      // Text format shouldn't be valid JSON
      expect(() => JSON.parse(result.output)).toThrow();
    });

    test('json format returns valid JSON', async () => {
      const result = await runWorkflow(TEST_WORKFLOW, {
        dryRun: true,
        format: 'json',
      });
      expect(() => JSON.parse(result.output)).not.toThrow();
    });
  });

  describe('no-color option', () => {
    test('respects noColor option', async () => {
      const result = await runWorkflow(TEST_WORKFLOW, {
        dryRun: true,
        noColor: true,
      });
      // Should not contain ANSI escape codes
      expect(result.output).not.toMatch(/\x1b\[/);
    });
  });
});
```

Adjust test expectations based on actual implementation output.
  </action>
  <verify>Tests pass: `bun test src/cli/run.test.ts`</verify>
  <done>All run command tests pass</done>
</task>

<task type="auto">
  <name>Task 3: Run full test suite</name>
  <files>N/A</files>
  <action>
Run the complete test suite to ensure no regressions:

```bash
bun test
```

Fix any failures in the run command implementation or tests.
  </action>
  <verify>All tests pass: `bun test`</verify>
  <done>Full test suite passes with no regressions</done>
</task>

</tasks>

<verification>
Run these commands to verify the test implementation:

1. **Run tests:**
   ```bash
   bun test src/cli/run.test.ts
   ```

2. **Check test coverage areas:**
   - File not found handling
   - Parse error handling
   - Validation error handling
   - Dry-run execution plan display
   - Config override parsing (single, multiple, JSON values)
   - Invalid config format error
   - Input JSON parsing
   - Invalid input JSON error
   - Output format (text/json)
   - No-color option

3. **Full test suite:**
   ```bash
   bun test
   ```

4. **Manual CLI verification:**
   ```bash
   # Should show execution plan
   bun src/cli/index.ts run --dry-run src/cli/fixtures/test-workflow.flow.md

   # Should show error
   bun src/cli/index.ts run --dry-run nonexistent.flow.md
   ```
</verification>

<success_criteria>
- [ ] Test workflow fixture created and validates
- [ ] run.test.ts exists with comprehensive test coverage
- [ ] All tests in run.test.ts pass
- [ ] Full test suite (bun test) passes with no regressions
- [ ] Tests cover: file handling, dry-run, config overrides, input parsing, output format, no-color
</success_criteria>

<output>
After completion, create `.planning/phases/08-full-cli/08-02-SUMMARY.md`
</output>
